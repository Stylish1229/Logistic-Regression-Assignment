{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression .\n",
        "- Logistic Regression is a classification algorithm used to predict categorical outcomes, especially binary (0 or 1). It uses the sigmoid function to convert the output into a probability between 0 and 1.\n",
        "\n",
        "- The main difference is : Linear Regression is used for predicting continuous values.\n",
        "- Logistic Regression is used for predicting class labels (like Yes/No or 0/1).\n",
        "\n"
      ],
      "metadata": {
        "id": "D_diPa3eMORU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the mathematical equation of Logistic Regression.\n",
        "- The mathematical equation of Logistic Regression is :\n",
        "\n",
        "$$\n",
        "P(y=1|x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_nx_n)}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $P(y=1|x)$ is the probability that the output is 1 given input $x$\n",
        "- $\\beta_0$ is the intercept (bias)\n",
        "- $\\beta_1, \\beta_2, ..., \\beta_n$ are the coefficients\n",
        "- $x_1, x_2, ..., x_n$ are the input features\n",
        "- $e$ is Euler's number (≈ 2.718)\n",
        "-This is also called the sigmoid function.\n",
        "- Let me know if you want a visual or simplified version too!\n",
        "\n"
      ],
      "metadata": {
        "id": "C2NXHYmaNarM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Why do we use the Sigmoid function in Logistic Regression.\n",
        "- We use the sigmoid function in Logistic Regression to convert the output of a linear equation into a probability between 0 and 1. This helps in classifying data into classes like 0 or 1.\n",
        "- It maps any real number to a value between 0 and 1, making it perfect for binary classification.\n",
        "\n"
      ],
      "metadata": {
        "id": "-2KDVZ8fSpOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the cost function of Logistic Regression.\n",
        "- The cost function of Logistic Regression is called Log Loss or Binary Cross-Entropy. It measures how well the predicted probabilities match the actual labels."
      ],
      "metadata": {
        "id": "KF5nrEJYTAU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Regularization in Logistic Regression? Why is it needed ?\n",
        "- Regularization is a technique used in Logistic Regression to prevent overfitting by adding a penalty term to the cost function.\n",
        "- It reduces the magnitude of model coefficients, which helps the model generalize better on new/unseen data.\n",
        "- There are two common types:\n",
        "\n",
        "- L1 Regularization (Lasso) - adds the absolute value of coefficients.\n",
        "- L2 Regularization (Ridge) - adds the square of coefficients.\n",
        "-  It is needed :  Without regularization, the model may fit the training data too well and perform poorly on test data (overfitting). Regularization helps improve model simplicity and accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "MTpS9576TV_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regression .\n",
        "- Ridge Regression (L2) :\n",
        "- Uses all features : Just makes the coefficients smaller.\n",
        "- Does not make any coefficient zero.\n",
        "- Lasso Regression (L1) :\n",
        "- Can make some coefficients exactly zero.\n",
        "- So, it also helps in feature selection.\n",
        "- Elastic Net Regression : It is a combination of Ridge and Lasso.\n",
        "- Shrinks coefficients and also removes some features.\n",
        "- Useful when there are many features or highly related features.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oYR2QJUXU5qM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When should we use Elastic Net instead of Lasso or Ridge ?\n",
        "- Elastic Net :\n",
        "- There are many correlated features : Lasso may select one feature from a group of correlated features, while Elastic Net can keep them together.\n",
        "- You need a balance between Lasso and Ridge : Elastic Net combines the benefits of both regularizations.\n",
        "- Lasso alone doesn't work well : If Lasso is too aggressive and removes too many features, Elastic Net can be a better choice.\n",
        "- When to use Lasso :\n",
        "- If you want automatic feature selection and expect that only a few features are important.\n",
        "-  When to use Ridge :\n",
        "- If all features are important and you want to reduce the impact of outliers or noise without eliminating any features.\n"
      ],
      "metadata": {
        "id": "Cz670ZYBWkdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the impact of the regularization parameter (λ) in Logistic Regression.\n",
        "- The regularization parameter (λ) controls the strength of regularization (either L1 or L2) in Logistic Regression.\n",
        "\n",
        "- High λ : Increases the regularization effect.\n",
        "- Forces the coefficients to be smaller, potentially leading to underfitting (model becomes too simple and fails to capture important patterns).\n",
        "\n",
        "- Low λ : Reduces the regularization effect.\n",
        "- Coefficients can be larger, potentially leading to overfitting (model becomes too complex and fits noise in the data).\n",
        "- Impact on Model :\n",
        "High λ → Simpler model, less prone to overfitting but might underfit.\n",
        "- Low λ → More complex model, higher risk of overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "kezK_XZvXujU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the key assumptions of Logistic Regression.\n",
        "- The key assumptions of Logistic Regression are :\n",
        "\n",
        "1. Binary Outcome (Dependent Variable) :\n",
        "   Logistic Regression assumes that the dependent variable (output) is binary (i.e., 0 or 1, Yes or No, etc.).\n",
        "\n",
        "2. Linear Relationship Between Independent Variables and Log-Odds:\n",
        "   It assumes a linear relationship between the independent variables and the log-odds of the dependent variable. This is why the linear equation is passed through the sigmoid function to convert it into probability.\n",
        "\n",
        "3. Independence of Observations :\n",
        "   Observations should be independent of each other. For example, no two data points should be related (i.e., no autocorrelation).\n",
        "\n",
        "4. No or Little Multicollinearity :\n",
        "   The independent variables should not be highly correlated with each other (i.e., no multicollinearity). High correlation between features can affect the model's performance.\n",
        "\n",
        "5. Large Sample Size:\n",
        "   Logistic Regression performs better with a large sample size, as the model needs a good amount of data to estimate the coefficients accurately.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GP09vYWcYlyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are some alternatives to Logistic Regression for classification tasks.\n",
        "- Here are some popular alternatives to Logistic Regression for classification tasks :\n",
        "1. Decision Trees :\n",
        "What it is: A tree-like model that splits the data into subsets based on feature values.\n",
        "2. When to use : When you want an easy-to-interpret model, especially when features are non-linear.\n",
        "Random Forest :\n",
        "- What it is : An ensemble of decision trees, where each tree makes a classification and the majority vote is the final prediction.\n",
        "- When to use : When you need more accuracy and robustness than a single decision tree. Great for large datasets and can handle non-linear relationships.\n",
        "3.  Support Vector Machine (SVM)\n",
        "- What it is: A model that finds the best hyperplane (decision boundary) to separate data points of different classes.\n",
        "- When to use : When you need a powerful classifier with high-dimensional data or when the classes are not linearly separable.\n",
        "\n",
        "4.  K-Nearest Neighbors (KNN)\n",
        "\n",
        "- What it is : A simple algorithm that classifies a data point based on the majority class of its nearest neighbors.\n",
        "- When to use : When the data is not linearly separable, or you want a non-parametric, simple method.\n",
        "\n",
        "5. Naive Bayes\n",
        "\n",
        "- What it is: A probabilistic classifier based on Bayes' Theorem, assuming feature independence.\n",
        "- When to use: When you have categorical data or want a model that performs well with less data.\n",
        "\n",
        "6. Gradient Boosting Machines (GBM)\n",
        "\n",
        "- What it is : An ensemble method that builds multiple trees sequentially, each trying to correct errors from the previous one.\n",
        "-  When to use : When you need high performance with complex, non-linear relationships. It is particularly effective in competitions and real-world tasks.\n",
        "\n",
        "7. Neural Networks\n",
        "\n",
        "-  What it is : A model inspired by the human brain, consisting of layers of neurons that can learn complex patterns in data.\n",
        "- When to use : For complex, large datasets, especially when there is a need for deep learning in tasks like image recognition, speech recognition, etc.\n",
        "\n",
        "-  Summary :\n",
        "\n",
        "- Logistic Regression : Simple and interpretable, good for linearly separable data.\n",
        "- Decision Trees/Random Forest : Better for non-linear data.\n",
        "- SVM : Powerful for high-dimensional or complex datasets.\n",
        "- KNN : Simple, non-parametric, works well when data is well separated.\n",
        "- Naive Bayes : Fast and efficient, good for text classification.\n",
        "- Gradient Boosting : High performance, but computationally more expensive.\n",
        "- Neural Networks: Best for large, complex datasets but requires more computational power.\n",
        "\n"
      ],
      "metadata": {
        "id": "4Da6DZV0ZULt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are Classification Evaluation Metrics.\n",
        "- Classification Evaluation Metrics are used to measure how well a classification model performs. Common metrics include :\n",
        "Accuracy, Precision,Recall,F1 Score,ROC-AUC"
      ],
      "metadata": {
        "id": "f-PPvEzpc47O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.  How does class imbalance affect Logistic Regression.\n",
        "- class imbalance affect Logistic Regression :\n",
        "Class imbalance can make Logistic Regression biased toward the majority class, leading to poor performanceon the minority class.\n",
        "As a result, the model may show high accuracy but low precision or recall for the minority class.\n",
        "\n"
      ],
      "metadata": {
        "id": "0z6547ZSduNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Hyperparameter Tuning in Logistic Regression.\n",
        "-  What is Hyperparameter Tuning in Logistic Regression :\n",
        "Hyperparameter tuning in Logistic Regression means finding the best values for parameters like regularization strength (λ or C) to improve model performance.\n",
        "- This is usually done using techniques like Grid Search or Cross-Validation.\n"
      ],
      "metadata": {
        "id": "PAmYRaR1eSk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are different solvers in Logistic Regression? Which one should be used .\n",
        "- Common solvers in Logistic Regression are :\n",
        "- liblinear - Good for small datasets, supports L1 and L2 regularization.\n",
        "- saga -  Works well with large datasets, supports L1, L2, and Elastic Net.\n",
        "- lbfgs - Fast and accurate for medium to large datasets, supports only L2.\n",
        "- newton-cg - Good for large datasets, supports only L2.\n",
        "- sag - Fast for large datasets, supports only L2.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P_5TiUoDexk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How is Logistic Regression extended for multiclass classification.\n",
        "- Logistic Regression is extended to multiclass classification using :\n",
        "\n",
        "1. One-vs-Rest (OvR) :\n",
        "- Trains one classifier per class.\n",
        "-  Each classifier predicts whether the sample belongs to its class or not.\n",
        "\n",
        "2. Multinomial (Softmax Regression) :\n",
        "- Uses a single model to directly predict the probabilities of all classes.\n",
        "- More accurate for multiclass tasks.\n",
        "- Use OvR for simplicity, and Multinomial when classes are related or for better performance.\n"
      ],
      "metadata": {
        "id": "wPWwvtNLfg2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the advantages and disadvantages of Logistic Regression .\n",
        "- The advantages and disadvantages of Logistic Regression :\n",
        "- Advantages :\n",
        "- Simple and easy to implement\n",
        "- Works well for binary classification\n",
        "- Outputs probabilities\n",
        "- Less prone to overfitting with regularization\n",
        "- Disadvantages :\n",
        "- Assumes linear relationship between features and log-odds\n",
        "- Not suitable for complex or non-linear problems\n",
        "- Performance drops with imbalanced data\n",
        "- Can't handle too many irrelevant features well\n",
        "\n"
      ],
      "metadata": {
        "id": "FIfvNtfRgfd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are some use cases of Logistic Regression.\n",
        "- Here are common use cases of Logistic Regression :\n",
        "- Email Spam Detection\n",
        "- Disease Prediction (e.g., diabetes, cancer)\n",
        "- Credit Card Fraud Detection\n",
        "- Customer Churn Prediction\n",
        "- Loan Default Prediction\n",
        "- Ad Click Prediction\n",
        "- It's mainly used for binary classification problems.\n"
      ],
      "metadata": {
        "id": "kRZ-q3G_h7sX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the difference between Softmax Regression and Logistic Regression.\n",
        "- The difference between Softmax Regression and Logistic Regression :\n",
        "- Logistic Regression is used for binary classification (2 classes).\n",
        "- Softmax Regression (also called Multinomial Logistic Regression) is used for multiclass classification (more than 2 classes).\n",
        "\n",
        "- Key Point :\n",
        "- Logistic Regression uses the sigmoid function.\n",
        "- Softmax Regression uses the softmax function to give probabilities for all classes.\n"
      ],
      "metadata": {
        "id": "ZE2idf_tiz89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification.\n",
        "- we choose between One-vs-Rest (OvR) and Softmax for multiclass classification\n",
        "- Use OvR (One-vs-Rest) when :\n",
        "- Simpler to implement.\n",
        "- Classes are well-separated.\n",
        "- You want to train separate models for each class.\n",
        "- Use Softmax (Multinomial Logistic Regression) when :\n",
        "- You want one single model for all classes.\n",
        "- Classes are related or overlapping.\n",
        "-  You need better probability estimates for each class.\n",
        "- Softmax generally performs better, but OvR is simpler and faster for small or less complex problems.\n"
      ],
      "metadata": {
        "id": "dtsV6IzMkWTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "- In logistic regression, coefficients represent the change in the log-odds of the outcome for a one-unit increase in the predictor variable, holding other variables constant. A positive coefficient indicates an increased likelihood of the outcome, while a negative coefficient suggests a decreased likelihood. Exponentiating the coefficient yields the odds ratio, indicating how the odds of the outcome change with a one-unit increase in the predictor. An odds ratio greater than 1 suggests higher odds, less than 1 suggests lower odds, and around 1 indicates little effect."
      ],
      "metadata": {
        "id": "2zHLQfQZlx-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy ?\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tT5XcLQOj5qf",
        "outputId": "73440d7d-5839-43ee-8f23-10f3e02df88d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy ?\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Binary classification only: use two classes (e.g., class 0 and 1)\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model with L1 penalty (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L1 Regularization: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN1KkqzxkrTv",
        "outputId": "39eb3f73-8210-4620-8d3a-aad5996cf7a6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L1 Regularization: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients?\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model with L2 penalty (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', multi_class='auto', max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L2 Regularization: {accuracy:.2f}\")\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"Model Coefficients (per class):\")\n",
        "print(model.coef_)\n",
        "\n",
        "# Print model intercept\n",
        "print(\"Model Intercept (per class):\")\n",
        "print(model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKzFAPaklDtV",
        "outputId": "5040c10b-547c-4f4a-a81a-3f8c62cc94b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L2 Regularization: 1.00\n",
            "Model Coefficients (per class):\n",
            "[[-0.39345607  0.96251768 -2.37512436 -0.99874594]\n",
            " [ 0.50843279 -0.25482714 -0.21301129 -0.77574766]\n",
            " [-0.11497673 -0.70769055  2.58813565  1.7744936 ]]\n",
            "Model Intercept (per class):\n",
            "[  9.00884295   1.86902164 -10.87786459]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet') ?\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Binary classification only (ElasticNet with 'saga' solver requires this for simplicity)\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model with Elastic Net penalty\n",
        "model = LogisticRegression(\n",
        "    penalty='elasticnet',\n",
        "    solver='saga',             # 'saga' is required for elasticnet\n",
        "    l1_ratio=0.5,              # Mixing parameter between L1 and L2 (0 = L2, 1 = L1)\n",
        "    max_iter=200\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with Elastic Net Regularization: {accuracy:.2f}\")\n",
        "\n",
        "# Print model coefficients and intercept\n",
        "print(\"Model Coefficients:\", model.coef_)\n",
        "print(\"Model Intercept:\", model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCQqzVr3lZAd",
        "outputId": "fdd6d127-aadc-4391-91c7-5218ae7edc1b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with Elastic Net Regularization: 1.00\n",
            "Model Coefficients: [[-0.11095479 -1.54448661  2.33796782  0.6056237 ]]\n",
            "Model Intercept: [-1.42685675]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'?\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset (3 classes: 0, 1, 2)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model using One-vs-Rest (OvR)\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Multiclass Logistic Regression Accuracy (OvR): {accuracy:.2f}\")\n",
        "\n",
        "# Print model coefficients and intercepts\n",
        "print(\"Model Coefficients (one row per class):\")\n",
        "print(model.coef_)\n",
        "\n",
        "print(\"Model Intercepts (one per class):\")\n",
        "print(model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K8xJe3mlrL2",
        "outputId": "6663b145-8244-469c-f69b-5cd7fddf2825"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multiclass Logistic Regression Accuracy (OvR): 1.00\n",
            "Model Coefficients (one row per class):\n",
            "[[ 0.3711229   1.409712   -2.15210117 -0.95474179]\n",
            " [ 0.49400451 -1.58897112  0.43717015 -1.11187838]\n",
            " [-1.55895271 -1.58893375  2.39874554  2.15556209]]\n",
            "Model Intercepts (one per class):\n",
            "[ 0.2478905   0.86408083 -1.00411267]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy ?\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for C and penalty\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],           # Regularization strength\n",
        "    'penalty': ['l1', 'l2'],               # Type of regularization\n",
        "    'solver': ['liblinear']                # 'liblinear' supports both 'l1' and 'l2'\n",
        "}\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the model with the best parameters\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = grid.best_estimator_.predict(X_test)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(\"Best Parameters Found:\", grid.best_params_)\n",
        "print(f\"Test Set Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOx3BDZVl762",
        "outputId": "c21f3739-12ce-4dbb-9442-dbef9ae42cf9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters Found: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Test Set Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7.Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy ?\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation and get accuracy scores\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Print accuracy for each fold\n",
        "print(\"Accuracy for each fold:\", scores)\n",
        "\n",
        "# Print average accuracy\n",
        "print(f\"Average Accuracy: {np.mean(scores):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37y-w7kym7GO",
        "outputId": "93768ef3-5fba-4f3e-9047-2c594a8b4c99"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for each fold: [1.         0.96666667 0.93333333 1.         0.93333333]\n",
            "Average Accuracy: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy?\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nLogistic Regression Accuracy: {accuracy:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuiN3CqMnRMf",
        "outputId": "7a22e898-4ca3-4012-f6be-fb6670a6c594"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Logistic Regression Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy ?\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the logistic regression model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "\n",
        "# Define the parameter distribution\n",
        "param_dist = {\n",
        "    'C': uniform(loc=0.01, scale=10),  # C values from 0.01 to 10.01\n",
        "    'penalty': ['l1', 'l2'],           # Regularization types\n",
        "    'solver': ['liblinear', 'saga']    # Solvers that support both l1 and l2\n",
        "}\n",
        "\n",
        "# Setup RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=10,          # Number of random combinations to try\n",
        "    scoring='accuracy',\n",
        "    cv=5,               # 5-fold cross-validation\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV to the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model and make predictions\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Print best hyperparameters and accuracy\n",
        "print(\"Best Parameters Found:\", random_search.best_params_)\n",
        "print(f\"Test Set Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIpJGJtSo32W",
        "outputId": "83855cbf-e258-4c1d-b1d7-1c7ddbf53eae"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters Found: {'C': np.float64(8.334426408004218), 'penalty': 'l2', 'solver': 'saga'}\n",
            "Test Set Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy ?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize base logistic regression model\n",
        "base_lr = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Wrap it with OneVsOneClassifier\n",
        "ovo_clf = OneVsOneClassifier(base_lr)\n",
        "\n",
        "# Train the OvO classifier\n",
        "ovo_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = ovo_clf.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"One-vs-One Logistic Regression Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3H73y8ApUkf",
        "outputId": "25b28638-16e1-4e82-ec6b-3bd1ac1a2f41"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One Logistic Regression Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification ?\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Load Iris dataset and filter for binary classification (class 0 and 1)\n",
        "iris = load_iris()\n",
        "X = iris.data[iris.target != 2]\n",
        "y = iris.target[iris.target != 2]\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names[:2])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix - Logistic Regression (Binary Classification)\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "QDgG8tjGpyeY",
        "outputId": "b70205cf-d57f-421c-ce68-4e4af18683e8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score ?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Load Iris dataset and filter for binary classification (class 0 and 1)\n",
        "iris = load_iris()\n",
        "X = iris.data[iris.target != 2]\n",
        "y = iris.target[iris.target != 2]\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Precision, Recall, F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the scores\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall:    {recall:.2f}\")\n",
        "print(f\"F1-Score:  {f1:.2f}\")\n",
        "\n",
        "# Optional: Detailed classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rx50JlJSqeno",
        "outputId": "f64e875b-205e-4e8c-8cba-8313195f163e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.00\n",
            "Recall:    1.00\n",
            "F1-Score:  1.00\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        12\n",
            "           1       1.00      1.00      1.00         8\n",
            "\n",
            "    accuracy                           1.00        20\n",
            "   macro avg       1.00      1.00      1.00        20\n",
            "weighted avg       1.00      1.00      1.00        20\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance ?\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Create an imbalanced dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=2,\n",
        "    n_redundant=10,\n",
        "    n_clusters_per_class=1,\n",
        "    weights=[0.9, 0.1],   # 90% of class 0, 10% of class 1 (imbalanced)\n",
        "    flip_y=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression WITHOUT class weights\n",
        "model_no_weights = LogisticRegression(max_iter=200)\n",
        "model_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = model_no_weights.predict(X_test)\n",
        "\n",
        "# Train Logistic Regression WITH class weights (balanced)\n",
        "model_weights = LogisticRegression(class_weight='balanced', max_iter=200)\n",
        "model_weights.fit(X_train, y_train)\n",
        "y_pred_weights = model_weights.predict(X_test)\n",
        "\n",
        "# Print accuracy and classification reports\n",
        "print(\"Without Class Weights:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_no_weights):.2f}\")\n",
        "print(classification_report(y_test, y_pred_no_weights))\n",
        "\n",
        "print(\"\\nWith Class Weights:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_weights):.2f}\")\n",
        "print(classification_report(y_test, y_pred_weights))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QccHbORmrEi3",
        "outputId": "13854c2d-dd20-4d6e-d076-c4afdd86a25a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without Class Weights:\n",
            "Accuracy: 0.99\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00       186\n",
            "           1       0.93      1.00      0.97        14\n",
            "\n",
            "    accuracy                           0.99       200\n",
            "   macro avg       0.97      1.00      0.98       200\n",
            "weighted avg       1.00      0.99      1.00       200\n",
            "\n",
            "\n",
            "With Class Weights:\n",
            "Accuracy: 0.96\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.96      0.98       186\n",
            "           1       0.67      1.00      0.80        14\n",
            "\n",
            "    accuracy                           0.96       200\n",
            "   macro avg       0.83      0.98      0.89       200\n",
            "weighted avg       0.98      0.96      0.97       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance?\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load Titanic dataset (using seaborn's dataset for convenience)\n",
        "import seaborn as sns\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Preview dataset\n",
        "print(titanic.head())\n",
        "\n",
        "# Select features and target\n",
        "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
        "target = 'survived'\n",
        "\n",
        "X = titanic[features]\n",
        "y = titanic[target]\n",
        "\n",
        "# Separate categorical and numerical columns\n",
        "cat_features = ['sex', 'embarked']\n",
        "num_features = ['pclass', 'age', 'sibsp', 'parch', 'fare']\n",
        "\n",
        "# Preprocessing pipelines for numeric and categorical data\n",
        "numeric_transformer = SimpleImputer(strategy='mean')\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, num_features),\n",
        "        ('cat', categorical_transformer, cat_features)\n",
        "    ])\n",
        "\n",
        "# Create a pipeline that first preprocesses the data then trains the model\n",
        "clf = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=200))\n",
        "])\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY755Nq6rthf",
        "outputId": "7bdd701a-16cc-4d01-a365-383f1e79706b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
            "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
            "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
            "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
            "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
            "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
            "\n",
            "     who  adult_male deck  embark_town alive  alone  \n",
            "0    man        True  NaN  Southampton    no  False  \n",
            "1  woman       False    C    Cherbourg   yes  False  \n",
            "2  woman       False  NaN  Southampton   yes   True  \n",
            "3  woman       False    C  Southampton   yes  False  \n",
            "4    man        True  NaN  Southampton    no   True  \n",
            "Accuracy: 0.80\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84       105\n",
            "           1       0.78      0.73      0.76        74\n",
            "\n",
            "    accuracy                           0.80       179\n",
            "   macro avg       0.80      0.79      0.80       179\n",
            "weighted avg       0.80      0.80      0.80       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# -------- Logistic Regression WITHOUT Scaling --------\n",
        "model_no_scaling = LogisticRegression(max_iter=200)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# -------- Logistic Regression WITH StandardScaler --------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_with_scaling = LogisticRegression(max_iter=200)\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "acc_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "# Print and compare accuracies\n",
        "print(f\"Accuracy without scaling: {acc_no_scaling:.2f}\")\n",
        "print(f\"Accuracy with scaling:    {acc_with_scaling:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYb22JfEsEWI",
        "outputId": "defa47fc-e1c3-4d41-87a5-7ceac7cddab6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 1.00\n",
            "Accuracy with scaling:    1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Iris dataset and filter for binary classification (class 0 and 1)\n",
        "iris = load_iris()\n",
        "X = iris.data[iris.target != 2]\n",
        "y = iris.target[iris.target != 2]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "auc_score = roc_auc_score(y_test, y_probs)\n",
        "print(f\"ROC-AUC Score: {auc_score:.2f}\")\n",
        "\n",
        "# Optional: Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
        "plt.plot(fpr, tpr, label=f\"ROC curve (area = {auc_score:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guess\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "yJi95i81tvmn",
        "outputId": "27dbcac0-8ae9-4b1f-e434-281b918f3a86"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 1.00\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression with C=0.5 (custom regularization)\n",
        "model = LogisticRegression(C=0.5, max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with C=0.5: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udjJnfbRuDmv",
        "outputId": "67873b67-0a4e-4429-a813-52714ed2ebf5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with C=0.5: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18. Write a Python program to train Logistic Regression and identify important features based on model coefficients ?\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# For simplicity, do binary classification: class 0 vs class 1 (filter out class 2)\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get model coefficients\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Pair features with their coefficients\n",
        "feature_importance = sorted(zip(feature_names, coefficients), key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "# Print features ranked by importance\n",
        "print(\"Feature importance based on coefficient magnitude:\")\n",
        "for feature, coef in feature_importance:\n",
        "    print(f\"{feature}: {coef:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4HDEoXxuhd3",
        "outputId": "5f5caa48-15d7-47af-cced-e6669a377d9c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importance based on coefficient magnitude:\n",
            "petal length (cm): 2.2123\n",
            "petal width (cm): 0.9274\n",
            "sepal width (cm): -0.8445\n",
            "sepal length (cm): 0.4625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score, classification_report\n",
        "\n",
        "# Load Iris dataset and filter for binary classification (class 0 and 1)\n",
        "iris = load_iris()\n",
        "X = iris.data[iris.target != 2]\n",
        "y = iris.target[iris.target != 2]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Cohen's Kappa Score\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa:.2f}\")\n",
        "\n",
        "# Optional: Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lh59WjwBu9Q2",
        "outputId": "b4aa492a-8962-4d1b-bdb5-b70db6da955f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 1.00\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        12\n",
            "           1       1.00      1.00      1.00         8\n",
            "\n",
            "    accuracy                           1.00        20\n",
            "   macro avg       1.00      1.00      1.00        20\n",
            "weighted avg       1.00      1.00      1.00        20\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification ?\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Load Iris dataset and filter for binary classification (class 0 and 1)\n",
        "iris = load_iris()\n",
        "X = iris.data[iris.target != 2]\n",
        "y = iris.target[iris.target != 2]\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute precision-recall curve and average precision score\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Plot Precisi\n"
      ],
      "metadata": {
        "id": "oUvMt8uzvVbX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy ?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List of solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Dictionary to store accuracies\n",
        "accuracies = {}\n",
        "\n",
        "for solver in solvers:\n",
        "    # Note: saga supports multinomial loss, liblinear does not\n",
        "    model = LogisticRegression(solver=solver, max_iter=200, multi_class='auto', random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies[solver] = acc\n",
        "\n",
        "# Print accuracies for each solver\n",
        "print(\"Accuracy comparison of Logistic Regression solvers:\")\n",
        "for solver, acc in accuracies.items():\n",
        "    print(f\"Solver: {solver} --> Accuracy: {acc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaFg6AJIvl2-",
        "outputId": "0b7194e6-adf3-430b-ddc3-663a98734bab"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy comparison of Logistic Regression solvers:\n",
            "Solver: liblinear --> Accuracy: 1.0000\n",
            "Solver: saga --> Accuracy: 1.0000\n",
            "Solver: lbfgs --> Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC) ?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef, classification_report\n",
        "\n",
        "# Load Iris dataset and filter for binary classification (class 0 and 1)\n",
        "iris = load_iris()\n",
        "X = iris.data[iris.target != 2]\n",
        "y = iris.target[iris.target != 2]\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Optional: print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QEtMBNRv60n",
        "outputId": "c5b5be1f-5df9-468e-c947-c02676a16027"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 1.00\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        12\n",
            "           1       1.00      1.00      1.00         8\n",
            "\n",
            "    accuracy                           1.00        20\n",
            "   macro avg       1.00      1.00      1.00        20\n",
            "weighted avg       1.00      1.00      1.00        20\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling ?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression on raw data\n",
        "model_raw = LogisticRegression(max_iter=200)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "acc_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Scale features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression on scaled data\n",
        "model_scaled = LogisticRegression(max_iter=200)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"Accuracy without scaling: {acc_raw:.4f}\")\n",
        "print(f\"Accuracy with scaling:    {acc_scaled:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckHUdvYVwRFv",
        "outputId": "d3f9b2bb-33ee-4570-f3aa-2aed13cc12e2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 1.0000\n",
            "Accuracy with scaling:    1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation ?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "\n",
        "# Define parameter grid for C (regularization strength)\n",
        "param_grid = {'C': [0.01, 0.1, 0.5, 1, 5, 10, 50, 100]}\n",
        "\n",
        "# Setup GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit grid search on training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameter C\n",
        "best_c = grid_search.best_params_['C']\n",
        "print(f\"Best C found by cross-validation: {best_c}\")\n",
        "\n",
        "# Evaluate best model on test data\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy with best C: {accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ3gWCMjwoKe",
        "outputId": "be036f45-b781-4b87-f944-87ed718babfc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C found by cross-validation: 1\n",
            "Test Accuracy with best C: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions ?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the trained model to a file\n",
        "joblib.dump(model, 'logistic_regression_model.joblib')\n",
        "print(\"Model saved to 'logistic_regression_model.joblib'\")\n",
        "\n",
        "# Load the model from the file\n",
        "loaded_model = joblib.load('logistic_regression_model.joblib')\n",
        "print(\"Model loaded from file\")\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of loaded model: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q20oZaJzxRt_",
        "outputId": "5ec90ce4-c241-47f9-bb46-4363cba7deac"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to 'logistic_regression_model.joblib'\n",
            "Model loaded from file\n",
            "Accuracy of loaded model: 1.0000\n"
          ]
        }
      ]
    }
  ]
}